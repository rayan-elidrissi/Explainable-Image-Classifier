# Explainable Image Classifier
Beyond model performance, image classifier that explains its decisions using interpretability techniques like Grad-CAM or LIME.

![Alt text](docs/image_classifier.png)

## Transparent Model
An example of critical use case where image classifier transparency is of utmost importance is in medical imaging diagnostics. In these context, AI models are used to classify medical images, such as X-rays, MRIs, or CT scans, to detect conditions like cancer, fractures, or neurological disorders. The decisions made by these models can have life-altering consequences, so it is crucial not only for the model to be highly accurate but also for its decision-making process to be transparent and explainable.

## Features
- A Convolutional Neural Network (CNN) model trained to classify images into different categories.
- Utilizes Grad-CAM (Gradient-weighted Class Activation Mapping) or LIME (Local Interpretable Model-agnostic Explanations) to visually explain the model's decision-making process.
- Includes a module to generate adversarial examples, testing the robustness of the model against adversarial attacks.
- Demonstrates how slight modifications to input images can change the model's prediction, providing insights into the decision boundaries of the classifier.

## Installation
```

```

## Usage
```

```

## Contributing
Contributions are welcome! Feel free to open an issue or submit a pull request.
